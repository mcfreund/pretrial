\documentclass{article}

% \renewcommand{\familydefault}{\sfdefault}  %% sans-serif
\usepackage{amsmath,amssymb}
\usepackage[margin =0.51in]{geometry}
\usepackage{enumitem}
\usepackage{parskip}


\begin{document}

\title{projection analysis}
\author{mike freund}
\date{\today}
\maketitle


\section*{intro}


\section*{notation}

\begin{itemize}
  \item vertex $v \text{ in } 1, \dots, V$
  \item TR $t \text{ in } 1, \dots, T$
  \item $\mathbf{b}$: \textit{training set} beta pattern vector of length $V$, condition indicated by subscript
  \item $\cdot'$: from \textit{test} set
  \item $\mathbf{e}_t$: residual vector (of length $V$) at TR = t
  \item $\mathbf{W}$: regularized inverse covariance matrix of residuals ($V \times V$)
\end{itemize}


\section*{within-run version}

First validate the method by examining the distribution about the discriminant \textit{within each run}.
This is perhaps the most `liberal' test as it does not require the discriminant to be aligned across scanning runs.

Calculate the mean pattern \textbf{m}:

\[
\mathbf{m} = ({\mathbf{b}_\mathit{(incon.)} + \mathbf{b}_\mathit{(congr.)}})/2
\]

Select a condition to represent the positive end of the discriminant.
Center the corresponding beta vector at the mean pattern and scale by the square of the length.
This gives the discriminant, \textbf{d}.

\[
\mathbf{d} = 
\frac
{\mathbf{b}_\mathit{(incon.)} - \mathbf{m}}
{||\mathbf{b}_\mathit{(incon.)} - \mathbf{m}||_2^2}
\]

Prewhiten and project a residual onto the discriminant.

\[\hat{e}_t = \mathbf{d} \mathbf{W} (\mathbf{e}_t - \mathbf{m})\]


unit: ratio of distance to hyperplane between test point and class centroid.

\textbf{how to validate (positive / negative control analyses)}
\begin{itemize}
  \item At post-trial TRs in ROIs, do residuals move along discriminant as function of previous trial type?
  \item Does this effect dissapear in non-ROIs?
  \item Detrend BOLD timeseries with baseline model of regression.
  Project detrended timeseries from ROIs and non-ROIs onto discriminant.
  Regress projection onto events design matrix.
  Does this recover trial type info?
  I.e., $\beta_\mathit{(incon.)} > 0$ and $\beta_\mathit{(congr.)} < 0$?
  (This projected timeseries could be used as a trial-by-trial measure, depending on its fidelity.)
  Can also assess correlation of recovered $\beta$ with \textbf{b}.
\end{itemize}

\section*{cross-validated version}

Next establish robustness to scanning run by cross-validating the discriminant \textit{across runs}.

\[\hat{e}_t' = \mathbf{d} \mathbf{W} (\mathbf{e}_t' - \mathbf{m}')\]

\textbf{validation}: repeat same validation analyses as above. Additionally:
\begin{itemize}
  \item calculate cosine angle between discriminants across runs. Hopefully positive.
  \item 
\end{itemize}






















% \section*{intro}
% 
% \subsection*{basic notation}
% 
% \begin{itemize}
%   \item vertex $v \text{ in } 1, \dots, V$
%   \item $\mathbf{a}$: pattern vector for condition a, of length $V$, within \textit{training set}
%   \item $\mathbf{b}$: ... for condition b
%   \item $\mathbf{x}'$: some observation (vector) within \textit{test} set
% \end{itemize}
% 
% 
% \section*{calculation}
% 
% \mathbf{1.} Select a condition: $\mathbf{a}$. This condition will define the `positive' end of the discriminant.
% 
% \textbf{2.} Subtract the \textit{mean pattern}, $\mathbf{m}$, from $\mathbf{a}$, and scale the resulting vector by this the square of its length.
% Call this $\textbf{d}$, for discriminant.
% 
% \[\mathbf{m} = (\mathbf{a} + \mathbf{b}) / 2\]
% 
% \[
% \mathbf{d} = 
% \frac
% {\mathbf{a} - \mathbf{m}}
% {||\mathbf{a} - \mathbf{m}||_2^2}
% \]
% 
% (Equivalently, scale the difference vector $\textbf{a} - \textbf{b}$ by two, then perform this scaling.)
% 
% Scaling by twice the length means that test points with projections (onto the discriminant) equal to 1, were equidistant from the 'hyperplane' as \textbf{a}.
% 
% \mathbf{3.} Similarly, remove mean pattern from test observation.
% 
% \[\mathbf{x}'_{-m} = \mathbf{x}' - \mathbf{m}\]
% 
% 
% \textbf{4.} Project test observation onto discriminant.
% 
% \[p_{x'} = \mathbf{x}'_{-m} \mathbf{d}\]



<<echo = FALSE>>=
# library(mikeutils)

# n <- 100
# 
# a <- runif(n)
# # b <- runif(n)
# b <- a + runif(n)
# m <- (a + b) / 2
# mu <- scale2unit(m)
# ac <- a - m
# bc <- b - m
# ssq(ac) == ssq(bc)
# # acs <- scale2unit(ac)
# # bcs <- scale2unit(bc)
# acs <- scale2unit(ac) * 1 / ssq(ac)
# bcs <- scale2unit(bc) * 1 / ssq(bc)
# all.equal(cosinesim(acs, bcs), -1)
# all.equal(c(acs %*% ac), 1)
# all.equal(c(bcs %*% bc), 1)
# 
# ## project new point p onto discriminant
# pa <- a * 10000
# pb <- b * 10000
# # cosinesim(a, b)
# # cosinesim(a, p)
# # cosinesim(b, p)
# 
# ## WRONG
# all(acs %*% pa > 0, acs %*% pb < 0)
# 
# ## RIGHT
# all(
#   acs %*% (pa - c(mu %*% pa %*% mu)) > 0,
#   acs %*% (pb - c(mu %*% pb %*% mu)) < 0,
#   bcs %*% (pa - c(mu %*% pa %*% mu)) < 0,
#   bcs %*% (pb - c(mu %*% pb %*% mu)) > 0
# )
# 
# 
# all.equal(scale2unit(ac) * 1 / ssq(ac), ac / ssq(ac)^2)
# 
# 
# 
# ## adaptively scaling the mean vector preserves length information of the 'test' dataset.
# ## all length info can easily be ignored, however, by scaling all input patterns to length 1.
# ## this procedure would give the same result.
# 
# cosinesim((a - c(mu %*% a %*% mu)), (b - c(mu %*% b %*% mu)))
# (a - m)
# 
# m %*% ac


@




\end{document}