\documentclass{article}

% \renewcommand{\familydefault}{\sfdefault}  %% sans-serif
\usepackage{amsmath,amssymb}
\usepackage[margin =0.51in]{geometry}
\usepackage{enumitem}
\usepackage{parskip}


\begin{document}

\title{projection analysis}
\author{mike freund}
\date{\today}
\maketitle

\section*{intro}
See bishop (p 182) for projection onto hyperplane, Izenman (p 242, eq 8.9) for expression of linear discriminant rule. (also p 107--119 HTF.)

\section*{notation}

\begin{itemize}
  \item vertex $v \text{ in } 1, \dots, V$
  \item TR $t \text{ in } 1, \dots, T$
  \item $\mathbf{m}_\mathit{(condition)}$: class centroid (beta pattern vector) of length $V$, \textit{condition} in \textit{incon.}, \textit{congr.}
  \item $\cdot'$: from test set
  \item $\mathbf{e}_t$: residual vector of length $V$, at TR = t
  \item $\mathbf{S^*}$: regularized covariance matrix of residuals ($V \times V$; asterisk indicates regularization)
\end{itemize}


\section*{within-run version}

First validate the method by examining the distribution about the discriminant \textit{within each run}.
This is perhaps the most `liberal' test as it does not require the discriminant to be aligned across scanning runs.

Select a condition to represent the positive end of the discriminant. 
Calculate the difference between class centroids and prewhiten.
This forms the discriminant function, $\mathbf{b}$.

\[\mathbf{b} = (\mathbf{m}_\mathit{incon.} - \mathbf{m}_\mathit{congr.}) \mathbf{S}^{*-1}\]

A residual is centered at the mean pattern and projected onto the discriminant.

\[\hat{e}_t = \mathbf{b} (\mathbf{e}_t - \mathbf{\bar{m}})\]

This will equal zero when $\mathbf{e}_t$ is equidistant between class means.

However the scale will depend on the length of the discriminant function.
Scaling this expression by the length of the discriminant function will yield the distance to hyperplane.
Scaling by the square of the length will render the units a ratio between distaince to hyperplane relative to distance between class mean.

\[
\hat{e}_t =
\frac{\mathbf{b} (\mathbf{e}_t - \mathbf{\bar{m}})}
{||\mathbf{b}||^2_2}
\]

E.g., $\hat{e}_t = 1$ would indicate that $\mathbf{e}_t$ was the same distance from hyperplane as the class centroid, $\mathbf{m}_\mathit{incon.}$.



% \section*{within-run version}
% 
% First validate the method by examining the distribution about the discriminant \textit{within each run}.
% This is perhaps the most `liberal' test as it does not require the discriminant to be aligned across scanning runs.
% 
% Calculate the mean pattern $\bar{\mathbf{m}}$.
% 
% \[
% \bar{\mathbf{m}} = ({\mathbf{m}_\mathit{(incon.)} + \mathbf{m}_\mathit{(congr.)}})/2
% \]
% 
% Select a condition to represent the positive end of the discriminant.
% Center the corresponding beta vector at the mean pattern.
% This gives the discriminant, \textbf{d}.
% 
% \[
% \mathbf{d} = 
% \frac
% {\mathbf{b}_\mathit{(incon.)} - \mathbf{m}}
% {||\mathbf{b}_\mathit{(incon.)} - \mathbf{m}||_2^2}
% \]
% 
% Prewhiten and project a residual onto the discriminant.
% 
% \[\hat{e}_t = \mathbf{d} \mathbf{W} (\mathbf{e}_t - \mathbf{m})\]
% 
% 
% Unit: ratio of distance to hyperplane between test point and class centroid.

\subsection*{validation: positive / negative control analyses}

Detrend BOLD timeseries with baseline model of regression.
Project detrended timeseries from ROIs and non-ROIs onto discriminant.
Regress projection onto events design matrix.\footnote{
Hierarchical regression \textit{may} be appropriate, if sufficiently complex model can be fit.
E.g., could collapse all $n$ trial type knots into single regressors, 
deviation coded, such that $\mathit{incon.} > 0$ and $\mathit{congr.} < 0$.
This would reduce to $n$ df per subject (estimated in random effects).
Perhaps not all of those need to be estimated.
}
Exmaine this projection to validate method.

\begin{itemize}
  \item Was trial info recovered? $\beta_\mathit{(incon.)} > 0$ and $\beta_\mathit{(congr.)} < 0$. Generalization across item types (PC50, Bias)?
  \item Sanity check: plot projection distributions per subject. Hopefully bimodal. Prelim look at inter-trial intervals --- where do these TRs fall on discriminant?
  \item Cosine similarity between recovered $\beta$ and \textbf{b}. Hopefully positive.
  \item In ROIs, at post-trial TRs, do residuals move along discriminant as function of previous trial type? Does this effect dissapear in non-ROIs?
  \item some form of autocorrelation analysis: how bad is problem?
\end{itemize}

Depending on success, these projections may be used to address the central question.

\section*{cross-validated version}

Next establish robustness to scanning run by cross-validating the discriminant \textit{across runs}.

This can either be done by assuming the same threshold (zero-point along the discriminant function)

\[\hat{e}_t' = \mathbf{b} (\mathbf{e}_t' - \bar{\mathbf{m}}) / ||\mathbf{b}||^2_2 \]

or by using the threshold estimated in the test run

\[\hat{e}_t'' = \mathbf{b} (\mathbf{e}_t' - \bar{\mathbf{m}}') / ||\mathbf{b}||^2_2\]

Using the test run threshold would be preferable if large, systematic (i.e., across all TRs) shifts in the mean pattern are expected between runs.

The adequacy of either threshold can be checked empirically: does the boundary drawn give good separation between projections?
If not, perhaps an optimal threshold can be selected.

\subsection*{validation: positive / negative control analyses}

\begin{itemize}
  \item Repeat same validation analyses as above. 
  \item Calculate cosine angle between discriminants across runs. Hopefully positive. (Can asess impact of prewhitening, too.)
\end{itemize}


\section*{analyses of theoretic interest}



% \section*{intro}
% 
% \subsection*{basic notation}
% 
% \begin{itemize}
%   \item vertex $v \text{ in } 1, \dots, V$
%   \item $\mathbf{a}$: pattern vector for condition a, of length $V$, within \textit{training set}
%   \item $\mathbf{b}$: ... for condition b
%   \item $\mathbf{x}'$: some observation (vector) within \textit{test} set
% \end{itemize}
% 
% 
% \section*{calculation}
% 
% \mathbf{1.} Select a condition: $\mathbf{a}$. This condition will define the `positive' end of the discriminant.
% 
% \textbf{2.} Subtract the \textit{mean pattern}, $\mathbf{m}$, from $\mathbf{a}$, and scale the resulting vector by this the square of its length.
% Call this $\textbf{d}$, for discriminant.
% 
% \[\mathbf{m} = (\mathbf{a} + \mathbf{b}) / 2\]
% 
% \[
% \mathbf{d} = 
% \frac
% {\mathbf{a} - \mathbf{m}}
% {||\mathbf{a} - \mathbf{m}||_2^2}
% \]
% 
% (Equivalently, scale the difference vector $\textbf{a} - \textbf{b}$ by two, then perform this scaling.)
% 
% Scaling by twice the length means that test points with projections (onto the discriminant) equal to 1, were equidistant from the 'hyperplane' as \textbf{a}.
% 
% \mathbf{3.} Similarly, remove mean pattern from test observation.
% 
% \[\mathbf{x}'_{-m} = \mathbf{x}' - \mathbf{m}\]
% 
% 
% \textbf{4.} Project test observation onto discriminant.
% 
% \[p_{x'} = \mathbf{x}'_{-m} \mathbf{d}\]



<<echo = FALSE>>=
# library(mikeutils)

# n <- 100
# 
# a <- runif(n)
# # b <- runif(n)
# b <- a + runif(n)
# m <- (a + b) / 2
# mu <- scale2unit(m)
# ac <- a - m
# bc <- b - m
# ssq(ac) == ssq(bc)
# # acs <- scale2unit(ac)
# # bcs <- scale2unit(bc)
# acs <- scale2unit(ac) * 1 / ssq(ac)
# bcs <- scale2unit(bc) * 1 / ssq(bc)
# all.equal(cosinesim(acs, bcs), -1)
# all.equal(c(acs %*% ac), 1)
# all.equal(c(bcs %*% bc), 1)
# 
# ## project new point p onto discriminant
# pa <- a * 10000
# pb <- b * 10000
# # cosinesim(a, b)
# # cosinesim(a, p)
# # cosinesim(b, p)
# 
# ## WRONG
# all(acs %*% pa > 0, acs %*% pb < 0)
# 
# ## RIGHT
# all(
#   acs %*% (pa - c(mu %*% pa %*% mu)) > 0,
#   acs %*% (pb - c(mu %*% pb %*% mu)) < 0,
#   bcs %*% (pa - c(mu %*% pa %*% mu)) < 0,
#   bcs %*% (pb - c(mu %*% pb %*% mu)) > 0
# )
# 
# 
# all.equal(scale2unit(ac) * 1 / ssq(ac), ac / ssq(ac)^2)
# 
# 
# 
# ## adaptively scaling the mean vector preserves length information of the 'test' dataset.
# ## all length info can easily be ignored, however, by scaling all input patterns to length 1.
# ## this procedure would give the same result.
# 
# cosinesim((a - c(mu %*% a %*% mu)), (b - c(mu %*% b %*% mu)))
# (a - m)
# 
# m %*% ac


@




\end{document}