---
title: 'Comparing measures of pattern similarity using DMCC2 Stroop'
author: "mike freund"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    highlight: zenburn
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
 
            autoNumber: "ams",
            formatNumber: function (n) {return +n}
      } 
  }
});
</script>


# 0. About

Here 12 different forms of similarity are considered.

* __3 measures of similarity__
    * linear **correlation**
    * **euclidean** distance
    * **standardized euclidean** distance

* __2 methods of estimation__
    * **vanilla** RSA: assessing similarity of patterns* *across scanning run**
    * **"cross-validated"** RSA: tweaking cross-run estimation procedure so that measures become *unbiased*

* __2 normalizing transforms__
    * **"raw"**, or un-normalized
    * spatially **prewhitened**, or "multivariate noise normalized"

Below, these things are described and references linked.

## measures of similarity and "methods of estimation"

### linear correlation

#### vanilla method

The correlation between two pattern vectors $x, y$ can be written

\[
r_\textit{vanilla} = \text{cor}(x, y) = \frac
{\text{Cov}(x, y)}
{\sqrt{\text{Var}(x)\text{Var}(y)}}
\]

i.e., the covariance of a measure standardized by the (rooted product of the) variances.

This is equivalent to writing

\[
  r_\textit{vanilla} = \frac{\text{Cov}(x, y)}{\sqrt{\text{Cov}(x, x)\text{Cov}(y, y)}}
\]

because the covariance of a measure with itself is the variance.

In the context of "vanilla" / cross-run RSA, the correlation of $x$ and $y$  is estimated _across scanning runs_ --- e.g., $x$ from run 1 and $y$ from run 2:

\begin{equation}

  r_\textit{vanilla} = \text{cor}(x_{(1)}, y_{(2)}) = 
  \frac{\text{Cov}(x_{(1)}, y_{(2)})}
  {\sqrt{\text{Cov}(x_{(1)}, x_{(1)})\text{Cov}(y_{(2)}, y_{(2)})}}
  
\end{equation}

Estimating across runs gives robustness to potential design artifacts stemming from temporal autocorrelation [(Alink et al, 2015)](https://www.biorxiv.org/content/10.1101/032391v2).

<!-- Because this yields two values --- $cor(x_1, y_2)$ and $cor(x_2, y_1)$ --- these are averaged together. -->

#### cross-validated method

To make the linear correlation unbiased, the [Spearman's correction for attenuation](https://en.wikipedia.org/wiki/Correction_for_attenuation) is applied.

The idea is that the maximum observable correlation between $x_{(1)}$ and $y_{(2)}$ is bounded by the (root product of their) reliabilities (lower reliability, lower correlation).
So, the observed correlation values are scaled by their maximum value, which is a function of their reliability.
Here, reliability is estimated as the cross-run covariance.

Essentially, this amounts to swapping one out of each of the measures in the denominator of __(1)__, $x_{(1)}$ and  $y_{(2)}$, with an independent 'copy' of itself, $x_{(2)}$ and $y_{(1)}$:

\begin{equation}

  r_\textit{cv} = \frac
  {\text{Cov}(x_{(1)}, y_{(1)})}
  {\sqrt{\text{Cov}(x_{(1)}, x_{(2)})\text{Cov}(y_{(1)}, y_{(2)})}}

\end{equation}

* see Appendix of [(Alink et al, 2015)](https://www.biorxiv.org/content/10.1101/032391v2) for expanded definition of cross-run correlation.

* because a covariance between independent measurements can be $\leq 0$, the denominator of the cross-validated correlation can be undefined (because of the root).

* this property is problematic for this measure, as even moderate amounts of noise can make this measure unstable and cause 'missing' data.

* I include the measure here for completeness but it was indeed very unstable.


### euclidean distance

#### vanilla

The vanilla / cross-run squared euclidean distance can be written

\[
d^2_\textit{vanilla} = \sum_{v = 1}^{V}(x_{v(1)} - y_{v(2)})^2
\]

i.e., the sum of squared differences between voxels $v$ in $1, \dots V$ in pattern $x$ from run 1 and pattern $y$ from run 2.

The subtraction operation above can also be thought of in vector form, where $x$ and $y$ are vectors $\mathbf{x}$ and $\mathbf{y}$, and $\mathbf{a} = \mathbf{x} - \mathbf{y}$ is the difference vector, or vector [pointing from $\mathbf{y}$ to $\mathbf{x}$](https://www.google.com/search?q=vector+subtraction&client=firefox-b-1-d&sxsrf=ALeKk00syNVDPLbGdMF_spS_wgfJXqp50A:1589025736959&source=lnms&tbm=isch&sa=X&ved=2ahUKEwih_9uU3qbpAhVGK80KHT5hB4EQ_AUoAXoECA4QAw&biw=2400&bih=1185#imgrc=Tjt-W0VN7s-a4M).
The squared euclidean distance can then be thought of as the squared _length_ of this difference vector.
[The squared length of a vector is just a vector multiplied by itself.]
So:

\begin{equation}
d^2_\textit{vanilla} = \mathbf{a}^2 = \mathbf{a} \cdot \mathbf{a} = \sum_{v = 1}^V a_v^2 =  \sum_{v = 1}^{V}(x_{v(1)} - y_{v(2)})^2
\end{equation}

#### cross-validated

To make this measure unbiased, the trick here is to perform the subtraction *within-run*, then multiply the difference vectors *between-run*.
This ensures that the two terms in the multiplication (two 'copies' of $\mathbf{a}$: $\mathbf{a}_{(1)}$ and $\mathbf{a}_{(2)}$) have independent errors.
Because they are independent, [these error terms cancel in the multiplication](https://doi.org/10.1016/j.neuroimage.2015.12.012), meaning that the resulting product (distance) reflects only the true distance.
<!-- swap one of the difference vectors above with an independent 'copy' of itself: -->

\begin{equation}

d^2_\textit{cv} = \mathbf{a}_{(1)} \cdot \mathbf{a}_{(2)} = \sum_{v = 1}^V a_{v{(1)}}a_{v{(2)}} = \sum_{v = 1}^{V}(x_{v(1)} - y_{v(1)})(x_{v(2)} - y_{v(2)})

\end{equation}


If there is no consistent difference between $x$ and $y$ across scanning runs, then the expected value of $d^2_\textit{cv}$ is zero.
(Note that, because $d^2_\textit{cv}$ can be negative, it is not square-rooted, but left as a squared euclidean distance.)

### standardized euclidean distance

#### vanilla

If the patterns $x$ and $y$ are z-score standardized, the squared euclidean's distance between them will be [equivalent to the linear correlation, within a scaling factor](https://arxiv.org/abs/1601.02213).

Let the tilde denote this standardization, e.g., 
\[
\widetilde{x}_{v(1)} = 
\frac{x_{v(1)} - \bar{x}_{v(1)}}
{\text{sd}(x_{v(1)})}
\]

so that

\begin{equation}

r_\textit{vanilla} \propto \widetilde{d}^2_\textit{vanilla} = \sum_{v = 1}^{V}(\widetilde{x}_{v(1)} - \widetilde{y}_{v(2)})^2

\end{equation}

For intuition, consider that the correlation is sensitive to the pattern "shape" (or, *angle* in vector space); scale (vector length) and mean differences (vector length along the unity line) are removed.
Likewise, the euclidean distance is sensitive to "shape" --- but also to scale, and to mean differences.
Removing scale and mean differences by z-score normalizing renders euclidean and correlation sensitive to the same information.

#### cross-validated

Standardized euclidean distance can be cross-validated in the same manner as euclidean's distance:

\begin{equation}

\widetilde{d}^2_\textit{cv} = \sum_{v = 1}^{V}(\widetilde{x}_{v(1)} - \widetilde{y}_{v(2)})(\widetilde{x}_{v(2)} - \widetilde{y}_{v(1)})

\end{equation}


## prewhitening

Any of these measures can be computed on spatially 'prewhitened' patterns.

### Description

Spatial prewhitening is a normalization procedure that shrinks the patterns along dimensions of correlated noise (thus stretching, relatively, along less noisy ones).

In the context of classification and Fisher's linear discriminant analysis, this whitening takes the form of normalizing by dividing patterns by the within-class (noise) covariance matrix, $\Sigma$ [(e.g.)](https://sthalles.github.io/fisher-linear-discriminant/), which is $V\times V$, and indicates how the class exemplars are distributed about their class centroids. (In LDA, all classess are assumed to share a common within-class covariance matrix.)
Fisher proved that using this transform maximized the between-class variance (the difference vector between class centroids) relative to the within-class variance, and he actually did so nonparametrically [(ESLII, p 110)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwii64jX76bpAhUKa80KHRQ7B4MQFjAAegQIAxAB&url=https%3A%2F%2Fweb.stanford.edu%2F~hastie%2FPapers%2FESLII.pdf&usg=AOvVaw25QCy16hNG1RTjwQm4qzz8).
After applying this sphering transform, Euclidean distances between observations [become Mahalanobis distances](https://en.wikipedia.org/wiki/Mahalanobis_distance#Definition_and_properties).

In the context of rapid event-related fMRI designs, problems in estimating $\Sigma$ arise.
There aren't enough exemplars (pattern estimates) of each class (condition) to get a good estimate of the across-voxel correlation structure.
But, a trick here is to use the residual timecourses from the GLM, not the within-class voxel-by-exemplar data matrices, to estimate $\Sigma$.
Given that there are typically more TRs within a run than voxels within a parcel, estimating $\Sigma$ is now much more tractable.
(Note that this is not the exact form of prewhitening derived by Fisher for LDA.
But nevertheless, the *prewhitened cross-valided euclidean distance* is what Walther et al. refer to as the *cross-validated mahalanobis*.)


### Estimation

I followed procedures outlined in [Diedrichsen et al (2016)](https://arxiv.org/abs/1607.01371).

Let $\mathbf{\epsilon}_r$ represent the matrix of residual timecourses for a given run $r$, parcel, and subject.
I.e., $\mathbf{\epsilon}_1$ is $TR \times V$ and corresponds to run 1.
The vertex-by-vertex covariance matrix of $\mathbf{\epsilon}_1$ is given by $\mathbf{\Sigma}_1 = \text{Cov}(\epsilon_1) = \mathbf{\epsilon}_1^T\mathbf{\epsilon}_1$.

* to curb overfitting (and potentially aid inversion), $\Sigma_1$ is regularized by shrinking it towards an identity matrix by a factor $\lambda$, i.e.: $\mathbf{\Sigma}^*_1 = \lambda \mathbf{I} + (1 - \lambda) \mathbf{\Sigma}_1$. This is a common procedure that makes LDA perform much better in high-dimensional scenarios (see, e.g. [Guo, Hastie, Tibshirani, 2006](https://doi.org/10.1093/biostatistics/kxj035)).
  
    * $\lambda$ can be estimated optimally [(Ledoit & Wolf, 2003)](https://repositori.upf.edu/bitstream/handle/10230/560/691.pdf); however for a first-pass analysis I used a healthy factor of $\lambda = 0.4$


* then $\Sigma_1$ is inverted: e.g.: $\mathbf{\Sigma}^{*-1}_1$


* this was done for both runs, and matrices were averaged across run $\mathbf{\Sigma}^{*-1} = (\mathbf{\Sigma}^{*-1}_1 + \Sigma^{*-1}_2)/2$.
This yields a single matrix $\Sigma^{*-1}$, to be applied to patterns of both runs.

    * however, an alternative method, which may give a lower variance estimate of $\Sigma^{*-1}$, would be to concatenate $\epsilon_1$ and $\epsilon_2$ along the time dimension, $\epsilon = \big(\begin{smallmatrix}
  \epsilon_1\\
  \epsilon_2
\end{smallmatrix}\big)$
, then use the concatenated matrix to estimate a single covariance matrix $\Sigma = \text{Cov}(\epsilon)$ across all timepoints from run 1 and run 2.


### Application


Prewhitening can be incorporated into any of the estimation procedures outlined above.
For example:

\[r_\textit{vanilla, prw} = \text{cor}(x_{(1)}\mathbf{\Sigma}^{*-1/2}, y_{(2)}\mathbf{\Sigma}^{*-1/2})\]

\[
d^2_\textit{vanilla, prw} = \mathbf{a} \mathbf{\Sigma}^{*-1} \mathbf{a}
\]

\[
d^2_\textit{cv, prw} = \mathbf{a}_{(1)} \mathbf{\Sigma}^{*-1} \mathbf{a}_{(2)}
\]


```{r setup, include = FALSE}

knitr::opts_chunk$set(
  cache = TRUE, echo = FALSE, warning = FALSE, message = FALSE,
  fig.align = 'center',
  fig.width = 11.5, fig.fullwidth = TRUE
)

set.seed(0)

library(here)
library(magrittr)
library(dplyr)
library(tidyr)
library(data.table)
library(mikeutils)
library(lme4)
library(lmerTest)
library(ggplot2)
library(ggbeeswarm)
library(ggridges)
library(viridis)
library(grid)
library(gridExtra)
library(cowplot)
library(cifti)
library(gifti)
library(abind)

source(here("src", "setup.R"))


## settings ----

theme_set(theme_minimal(base_size = 14))


## data ----

## parcel-wise similarity matrices

## vanilla RSA

vanil <- 
  abind(
    abind(
      readRDS(here("out", "rsa", "observed", "rmatrix_vanilla_corr_shaefer400_baseline_Congruency_EVENTS_censored.rds")),
      readRDS(here("out", "rsa", "observed", "rmatrix_vanilla_eucl_shaefer400_baseline_Congruency_EVENTS_censored.rds")),
      readRDS(here("out", "rsa", "observed", "rmatrix_vanilla_neuc_shaefer400_baseline_Congruency_EVENTS_censored.rds")),
      rev.along = 0
    ),
    abind(
      readRDS(here("out", "rsa", "observed", "rmatrix_vanilla_corr_shaefer400_proactive_Congruency_EVENTS_censored.rds")),
      readRDS(here("out", "rsa", "observed", "rmatrix_vanilla_eucl_shaefer400_proactive_Congruency_EVENTS_censored.rds")),
      readRDS(here("out", "rsa", "observed", "rmatrix_vanilla_neuc_shaefer400_proactive_Congruency_EVENTS_censored.rds")),
      rev.along = 0
    ),
    rev.along = 0
  )

names(dimnames(vanil)) <- c(".row", ".col", "norma", "knot", "parcel", "subj", "measure", "session")
dimnames(vanil)$measure <- c("corr", "eucl", "neuc")
dimnames(vanil)$session <- c("baseline", "proactive")

cross <- 
  abind(
    abind(
      readRDS(here("out", "rsa", "observed", "rmatrix_crossva_corr_shaefer400_baseline_Congruency_EVENTS_censored.rds")),
      readRDS(here("out", "rsa", "observed", "rmatrix_crossva_eucl_shaefer400_baseline_Congruency_EVENTS_censored.rds")),
      readRDS(here("out", "rsa", "observed", "rmatrix_crossva_neuc_shaefer400_baseline_Congruency_EVENTS_censored.rds")),
      rev.along = 0
    ),
    abind(
        readRDS(here("out", "rsa", "observed", "rmatrix_crossva_corr_shaefer400_proactive_Congruency_EVENTS_censored.rds")),
        readRDS(here("out", "rsa", "observed", "rmatrix_crossva_eucl_shaefer400_proactive_Congruency_EVENTS_censored.rds")),
        readRDS(here("out", "rsa", "observed", "rmatrix_crossva_neuc_shaefer400_proactive_Congruency_EVENTS_censored.rds")),
        rev.along = 0
      ),
    rev.along = 0
  )

names(dimnames(cross)) <- c(".row", ".col", "norma", "knot", "parcel", "subj", "measure", "session")
dimnames(cross)$measure <- c("corr", "eucl", "neuc")
dimnames(cross)$session <- c("baseline", "proactive")

## filter subjs by those with data

has.stats <- apply(vanil, "subj", function(.) !any(is.na(c(.))))
subjs.with.stats <- names(has.stats)[has.stats]

## TODO
## these subjects do not have stats
# setdiff(c(subjs.analysis, subjs.development), subjs.with.stats)
## could be due to
##  - no 3d+t gifti -> have RAs run
##  - no beta gifti -> diagnose problem and fit

subjs.analysis <- intersect(subjs.analysis, subjs.with.stats)
subjs.development <- intersect(subjs.development, subjs.with.stats)
subjs.bad <- intersect(subjs.bad, subjs.with.stats)

vanil <- vanil[, , , , , subjs.analysis, , ]
cross <- cross[, , , , , subjs.analysis, , ]

## palettes ----

colors.norma <- c(raw = "grey30", prw = "firebrick")
# colors.glm <- c("2tr1knot" = "#d95f02", "1tr1knot" = "#1b9e77")
colors.celltype <- c(I_I = "#e41a1c", C_C = "#377eb8", I_C = "#4daf4a")

## transformations

vanil[, , , , , , "corr", ] <- atanh(vanil[, , , , , , "corr", ])  ## fisher transform
vanil[, , , , , , c("eucl", "neuc"), ] <- -vanil[, , , , , , c("eucl", "neuc"), ]  ## negate for interpretability
cross[, , , , , , c("eucl", "neuc"), ] <- -cross[, , , , , , c("eucl", "neuc"), ]
# cross.pro[, , , , , , c("eucl", "neuc")] <- -cross.pro[, , , , , , c("eucl", "neuc")]


## extract target knot ----

vanil <- vanil[, , , "knot2", , , , ]
cross <- cross[, , , "knot2", , , , ]

# cross <- cross[, , , c("knot3", "knot4"), , , ] %>% apply(c(".row", ".col", "norma", "parcel", "subj", "measure"), mean)
# vanil <- vanil[, , , c("knot3", "knot4"), , , ] %>% apply(c(".row", ".col", "norma", "parcel", "subj", "measure"), mean)


```


# 1. Impact of prewhitening

* baseline PC50 only

* define function $\mathit{cell}(\text{rowname}, \text{colname})$: takes row and column names as input, returns value of cell

* vanilla contrast: 

\[
\text{contrast}_\textit{vanilla} = 
\mathit{cell}(\text{PC50InConRun1}, \text{PC50InConRun2}) -
[\mathit{cell}(\text{PC50InConRun1}, \text{PC50ConRun2}) +
  \mathit{cell}(\text{PC50InConRun2}, \text{PC50ConRun1})]/2
\]

This is tested against zero (over subjects): $\text{contrast}_\textit{vanilla} > 0$

* cross-validated contrast:

\[
\text{contrast}_\textit{cross-validated} = \mathit{cell}(\text{PC50InCon}, \text{PC50Con})
\]

The invidiual cells themselves represent contrasts.

These can be tested against zero (over subjects): $\text{contrast}_\textit{cross-validated} > 0$



```{r prewhitening_wrangle}

## vanilla measures ----

inds.pc50.run1 <- grep("PC50.*_run1", conds.run)
inds.pc50.run2 <- grep("PC50.*_run2", conds.run)

vanil.pc50 <- 
  
  vanil[5:8, 1:4, , , , , "baseline"] %>%  ## btw run only

  reshape2::melt(value.name = "simil") %>% 
  
  group_by(norma, parcel, subj) %>%   ## standardize within subjects
  mutate(simil.sd = simil / sd(simil), method = "vanil")

## get only pc50 and create celltype col

vanil.pc50$.row <- gsub("_run.$", "", vanil.pc50$.row)
vanil.pc50$.col <- gsub("_run.$", "", vanil.pc50$.col)

vanil.pc50 <- vanil.pc50 %>% filter(grepl("PC50", .row), grepl("PC50", .col))

vanil.pc50$celltype <- ifelse(
  grepl("PC50InCon", vanil.pc50$.row) & grepl("PC50InCon", vanil.pc50$.col), "I_I",
  ifelse(
    grepl("PC50Con", vanil.pc50$.row) & grepl("PC50Con", vanil.pc50$.col), "C_C", "I_C"
  )
)

vanil.pc50 <- vanil.pc50 %>%
  
  group_by(subj, parcel, celltype, norma, measure) %>%
  summarize(simil = mean(simil), simil.sd = mean(simil.sd)) %>%  ## average within celltype
  
  group_by(subj, parcel, norma, measure) %>%  ## perform contrast
  summarize(
    simil = simil[celltype == "I_I"] - simil[celltype == "I_C"],
    simil.sd = simil.sd[celltype == "I_I"] - simil.sd[celltype == "I_C"]
    )

## crossvalidated measures ----

## standardize measures within subjects

## correlation
cross.corr <- cross[, , , , , "corr", "baseline"] %>% 
  reshape2::melt(value.name = "simil") %>% 
  mutate(method = "cross", measure = "corr")
cross.corr <- cross.corr %>% group_by(norma, parcel, subj) %>% mutate(simil.sd = simil / sd(simil))

## euclidean and normalized euclidean
cross.eucls <- -cross[, , , , , c("eucl", "neuc"), "baseline"] %>%  ## un-negate (larger distance, more different!)
  reshape2::melt(value.name = "simil") %>%
  mutate(method = "cross")
cross.eucls <- cross.eucls %>% group_by(norma, parcel, subj) %>% mutate(simil.sd = simil / sd(simil))

## get pc50 only and bind

cross.pc50 <- bind_rows(cross.corr, cross.eucls)
cross.pc50 <- cross.pc50[cross.pc50$.row == "PC50InCon" & cross.pc50$.col == "PC50Con", ]

## bind ----

rsa.pc50 <- bind_rows(
  vanil.pc50 %>% mutate(method = "vanil"),
  cross.pc50 %>% select(-.row, -.col)
)


## take out trash ----

rm(vanil.pc50, cross.corr, cross.eucls, cross.pc50)
gc()


## add factor cols ----

## bind and add factor cols

rsa.pc50$network <- ""  ## add networks
for (network.i in networks) rsa.pc50$network[grepl(network.i, rsa.pc50$parcel)] <- network.i

rsa.pc50$parcel.num <- match(rsa.pc50$parcel, parcellation$key)

```

## stats

```{r prewhitening_mean_contrasts}

rsa.pc50 %>%
  
  filter(parcel.num %in% dmcc34) %>%
  group_by(subj, norma, measure, method) %>%
  summarize(simil = mean(simil)) %>%
  
  ggplot(aes(norma, simil, fill = norma)) +
  geom_hline(yintercept = 0) +
  geom_line(aes(group = subj), alpha = 0.2) +
  geom_boxplot(notch = TRUE, width = 0.25) +
  
  facet_wrap(vars(method, measure), scales = "free_y") +
  
  scale_fill_manual(values = colors.norma) +
  
  theme_light() +
  theme(legend.position = "none")

rsa.pc50 %>%
  
  filter(parcel.num %in% dmcc34) %>%
  group_by(subj, norma, measure, method) %>%
  summarize(simil.sd = mean(simil.sd)) %>%
  
  ggplot(aes(norma, simil.sd, fill = norma)) +
  geom_hline(yintercept = 0) +
  geom_line(aes(group = subj), alpha = 0.2) +
  geom_boxplot(notch = TRUE, width = 0.25) +
  
  facet_wrap(vars(method, measure), scales = "free_y") +
  
  scale_fill_manual(values = colors.norma) +
  
  theme_light() +
  theme(legend.position = "none")
  
# rsa.pc50 %>%
#   
#   filter(parcel.num %in% dmcc34) %>%
#   group_by(norma, measure, method, subj) %>%
#   summarize(simil = mean(simil)) %>%
#   mutate(simil = simil / sd(simil)) %>%
#   
#   ggplot(aes(norma, simil, fill = norma)) +
#   geom_hline(yintercept = 0) +
#   geom_line(aes(group = subj), alpha = 0.2) +
#   geom_boxplot(notch = TRUE, width = 0.25) +
#   
#   facet_wrap(vars(method, measure), scales = "free_y") +
#   
#   scale_fill_manual(values = colors.norma) +
#   
#   theme_light() +
#   theme(legend.position = "none")


rsa.pc50 %>%
  
  group_by(parcel, parcel.num, norma, measure, method, subj) %>%
  summarize(simil = mean(simil.sd)) %>%
  
  summarize(
    ssr = wilcox.test(simil, alternative = "greater")$statistic,
    p.value = wilcox.test(simil, alternative = "greater")$p.value
    ) %>%
  group_by(parcel, parcel.num, measure, method) %>%
  mutate(is.sig = any(p.value < 0.05), p.value = NULL) %>%
  
  pivot_wider(names_from = "norma", values_from = "ssr") %>%
  
  ggplot(aes(raw, prw, fill = parcel.num %in% dmcc34)) +
  geom_abline() +
  geom_hline(yintercept = 2) +
  geom_vline(xintercept = 2) +
  # geom_point(shape = 21, color = "white", size = 2) +
  geom_point(aes(alpha = is.sig), shape = 21, color = "white", size = 2) +
  
  facet_grid(vars(method), vars(measure)) +
  scale_fill_manual(values = c("TRUE" = "red", "FALSE" = "black")) +
  scale_alpha_manual(values = c("TRUE" = 1, "FALSE" = 0.2)) +
  
  theme_light() +
  theme(legend.position = "none")


```



```{r prewhitening_brains}

onesamp <- rsa.pc50 %>%
  group_by(parcel, norma, measure, method, network, parcel.num) %>%
  summarize(
    m = mean(simil.sd),
    ssr = wilcox.test(simil.sd, alternative = "greater")$statistic,
    p.value = wilcox.test(simil.sd, alternative = "greater")$p.value
    ) %>%
  group_by(norma, measure, method, network) %>%
  mutate(p.fdr = p.adjust(p.value, method = "fdr"))

paired <- rsa.pc50 %>%
  group_by(parcel, norma, measure, method, network, parcel.num) %>%
  mutate(simil.sd = simil.sd / sd(simil.sd)) %>%
  select(-simil) %>%
  pivot_wider(names_from = "norma", values_from = "simil.sd") %>%
  summarize(
    m = mean(prw) - mean(raw),
    ssr = wilcox.test(prw, raw, paired = TRUE)$statistic,
    p.value = wilcox.test(prw, raw, paired = TRUE)$p.value,
    ) %>%
  group_by(measure, method, network) %>%
  mutate(p.fdr = p.adjust(p.value, method = "fdr"))

# table(interaction(a$norma, a$measure, a$method), a$p.fdr < 0.05)

## add cols for plotting

onesamp %<>%
  mutate(
    hemi = substr(parcel, 1, 1),
    num.roi = match(parcel, parcellation$key)
    )

paired %<>%
  mutate(
    hemi = substr(parcel, 1, 1),
    num.roi = match(parcel, parcellation$key)
    )

```


## brains

* the t statistics from above are plotted on brains here.

* thresholded at FDR-corrected whole-brain $\alpha = 0.05$

### one-sample wilcox stats

#### vanilla measures

##### correlation, raw

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "vanil", measure == "corr", norma == "raw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### correlation, prewhitened

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "vanil", measure == "corr", norma == "prw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### euclidean, raw

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "vanil", measure == "eucl", norma == "raw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### euclidean, prewhitened

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "vanil", measure == "eucl", norma == "prw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```


##### standardized euclidean, raw

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "vanil", measure == "neuc", norma == "raw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### standardized euclidean, prewhitened

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "vanil", measure == "neuc", norma == "prw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

#### cross-validated measures

##### correlation, raw

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "cross", measure == "corr", norma == "raw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### correlation, prewhitened

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "cross", measure == "corr", norma == "prw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### euclidean, raw

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "cross", measure == "eucl", norma == "raw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### euclidean, prewhitened

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "cross", measure == "eucl", norma == "prw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### standardized euclidean, raw

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "cross", measure == "neuc", norma == "raw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### standardized euclidean, prewhitened

```{r, fig.height = 7, fig.width = 11}

onesamp %>%
  filter(method == "cross", measure == "neuc", norma == "prw") %>%
  mutate(ssr = ifelse(p.fdr < 0.05, ssr, 0)) %>%
  build_overlay("ssr", template = schaefer) %>%
  plot_surface(underlay = hcp)

```


### two-sample, paired wilcox stats

* plotted: standardized mean difference between prewhitened and raw measures (prw - raw), within subject

* thresholded at uncorrected p = 0.05

#### vanilla measures

##### correlation

```{r, fig.height = 7, fig.width = 11}

paired %>%
  filter(method == "vanil", measure == "corr") %>%
  mutate(m = ifelse(p.value < 0.05, m, 0)) %>%
  build_overlay("m", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### euclidean

```{r, fig.height = 7, fig.width = 11}

paired %>%
  filter(method == "vanil", measure == "eucl") %>%
  mutate(m = ifelse(p.value < 0.05, m, 0)) %>%
  build_overlay("m", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### standardized euclidean

```{r, fig.height = 7, fig.width = 11}

paired %>%
  filter(method == "vanil", measure == "neuc") %>%
  mutate(m = ifelse(p.value < 0.05, m, 0)) %>%
  build_overlay("m", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

#### cross-validated measures

##### correlation

```{r, fig.height = 7, fig.width = 11}

paired %>%
  filter(method == "cross", measure == "corr") %>%
  mutate(m = ifelse(p.value < 0.05, m, 0)) %>%
  build_overlay("m", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### euclidean

```{r, fig.height = 7, fig.width = 11}

paired %>%
  filter(method == "cross", measure == "eucl") %>%
  mutate(m = ifelse(p.value < 0.05, m, 0)) %>%
  build_overlay("m", template = schaefer) %>%
  plot_surface(underlay = hcp)

```

##### standardized euclidean

```{r, fig.height = 7, fig.width = 11}

paired %>%
  filter(method == "cross", measure == "neuc") %>%
  mutate(m = ifelse(p.value < 0.05, m, 0)) %>%
  build_overlay("m", template = schaefer) %>%
  plot_surface(underlay = hcp)

```


# 2. Impact of cross-validation on sensitivity to number of trials

### Mean (whole-brain) similarity matrices

_cf._, the 'vanilla' matrices displayed in the __./rois_rsa_vanilla/.__ analyses

```{r crossvalidated_matrices, fig.height = 10}

allsim <- apply(cross, c(".row", ".col", "norma", "measure", "session"), mean)


grid.arrange(
  
  allsim[, , "raw", "corr", "baseline"] %>% plot_matrix + labs(title = "correlation, bas"),
  allsim[, , "raw", "corr", "proactive"] %>% plot_matrix + labs(title = "correlation, pro"),
  
  allsim[, , "raw", "eucl", "baseline"] %>% plot_matrix + labs(title = "euclidean, bas"),
  allsim[, , "raw", "eucl", "proactive"] %>% plot_matrix + labs(title = "euclidean, pro"),
  
  allsim[, , "raw", "neuc", "baseline"] %>% plot_matrix + labs(title = "standardized euclidean, bas"),
  allsim[, , "raw", "neuc", "proactive"] %>% plot_matrix + labs(title = "standardized euclidean, pro"),
  
  ncol = 2
  
)

```


**a rough analysis of sensitivity of measures to trial balancing**

* focus on proactive data: not much going on in terms of observed stroop effects

* BiasInCon in proactive has 54 trials per run

* all other trial types (PC50Con, PC50InCon, biasCon), in proactive have 18 trials per run

* If a measure is sensitive to trial counts, BiasInCon should correlate more strongly with all other patterns, regardless of pc / congruency.

* Get average/sum of similarities of biasInCon and PC50InCon to congruent patterns:

\[\text{biasInCon}\sim\text{allCon} = \mathit{simil}(\text{BiasInCon}, \text{PC50Con}) + \mathit{simil}(\text{BiasInCon}, \text{biasCon})\]
\[\text{PC50InCon}\sim\text{allCon} = \mathit{simil}(\text{PC50InCon}, \text{PC50Con}) + \mathit{simil}(\text{PC50InCon}, \text{biasCon})\]

    * these indicate the average *cross-congruency* similarity for biasInCon and PC50InCon
    
* contrast: $(\text{biasInCon}\sim\text{allCon}) - (\text{PC50InCon}\sim\text{allCon})$?
    * will be positive if measure sensitive to number of trials.


```{r crossvalidated_stats}

## wrangle ----

## vanilla measures

vanil.d <- vanil[1:4, 5:8, "raw", , , , ] %>% reshape2::melt(value.name = "simil") %>% mutate(method = "vanil")

vanil.d$.row <- gsub("_run.$", "", vanil.d$.row)
vanil.d$.col <- gsub("_run.$", "", vanil.d$.col)

vanil.d$pc <- ifelse(
  grepl("PC50", vanil.d$.row) & grepl("PC50", vanil.d$.col), "pc50_pc50",
  ifelse(
    grepl("bias", vanil.d$.row) & grepl("bias", vanil.d$.col), "bias_bias",
    "pc50_bias"
  )
)
vanil.d$congruency <- ifelse(
  grepl("InCon", vanil.d$.row) & grepl("InCon", vanil.d$.col), "I_I",
  ifelse(
    grepl("biasCon|PC50Con", vanil.d$.row) & grepl("biasCon|PC50Con", vanil.d$.col), "C_C",
    "I_C"
  )
)

vanil.d <- vanil.d %>% filter(congruency == "I_C")

vanil.d$condition <- ifelse(
    grepl("biasInCon", vanil.d$.row) | grepl("biasInCon", vanil.d$.col), "biasI_C",
    ifelse(
      grepl("PC50InCon", vanil.d$.row) | grepl("PC50InCon", vanil.d$.col), "pc50I_C", NA
    )
)

# sum(is.na(vanil.d$condition))

vanil.d <- vanil.d %>%
  group_by(parcel, subj, measure, session, method, condition) %>%
  summarize(simil = mean(simil))  ## summarize by condiiton


## cross-validated measures

cross.d <- cross[, , "raw", , , , ] %>% reshape2::melt(value.name = "simil") %>% mutate(method = "cross")

cross.d$pc <- ifelse(
  grepl("PC50", cross.d$.row) & grepl("PC50", cross.d$.col), "pc50_pc50",
  ifelse(
    grepl("bias", cross.d$.row) & grepl("bias", cross.d$.col), "bias_bias",
    "pc50_bias"
  )
)
cross.d$congruency <- ifelse(
  grepl("InCon", cross.d$.row) & grepl("InCon", cross.d$.col), "I_I",
  ifelse(
    grepl("biasCon|PC50Con", cross.d$.row) & grepl("biasCon|PC50Con", cross.d$.col), "C_C",
    "I_C"
  )
)

cross.d <- cross.d %>% filter(congruency == "I_C")

cross.d$condition <- ifelse(
    grepl("biasInCon", cross.d$.row) | grepl("biasInCon", cross.d$.col), "biasI_C",
    ifelse(
      grepl("PC50InCon", cross.d$.row) | grepl("PC50InCon", cross.d$.col), "pc50I_C", NA
    )
)

# sum(is.na(cross.d$condition))

cross.d <- cross.d %>%
  group_by(parcel, subj, measure, session, method, condition) %>%
  summarize(simil = mean(simil))  ## summarize by condiiton

## bind

d <- bind_rows(cross.d, vanil.d)

## stats

d.stats <- d %>%
  
  .[.$session == 'proactive', ] %>%
  
  pivot_wider(names_from = "condition", values_from = "simil") %>%
  group_by(parcel, measure, method) %>%
  summarize(
    statistic = t.test(biasI_C, pc50I_C, paired = TRUE)$statistic,
    p.value = t.test(biasI_C, pc50I_C, paired = TRUE)$p.value
    ) %>%
  group_by(measure, method) %>%
  mutate(p.fdr = p.adjust(p.value, method = "fdr"))


```



```{r crossvalidated_plots}

d %>%
  
  .[.$session == 'proactive', ] %>%
  
  group_by(subj, measure, session, condition, method) %>%
  summarize(simil = mean(simil)) %>%
  pivot_wider(names_from = "condition", values_from = "simil") %>%
  
  ggplot(aes(biasI_C, pc50I_C)) +
  geom_abline() +
  geom_point(shape = 21, fill = "black", color = "grey50", size = 2) +
  
  facet_wrap(vars(method, measure), scales = "free") +
  labs(
    title = "cross-congruency similarities per subject (aggregated across all parcels, proactive)",
    x = "biasInCon~allCon",
    y = "PC50InCon~allCon",
    caption = "line is unity"
    )


# d.stats %>%
#   
#   ggplot(aes(method, statistic)) +
#   geom_boxplot(aes(fill = measure), position = position_dodge())

d.stats %>%
  
  select(-p.value, -p.fdr) %>%
  pivot_wider(names_from = "method", values_from = "statistic") %>%
  
  ggplot(aes(cross, vanil)) +
  geom_abline() +
  geom_point(aes(color = match(parcel, parcellation$key) %in% dmcc34)) +
  geom_point(
    data = . %>% group_by(measure) %>% summarize(cross = mean(cross), vanil = mean(vanil)),
    size = 5 
  ) +
  
  facet_wrap(vars(measure), scales = "free") +
  
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "grey50"), name = "in dmcc34") +

  labs(
    title = "cross-congruency similarity CONTRAST per parcel (paired sample t-test over subjects, proactive)",
    x = "cross-validated methods",
    y = "vanilla methods",
    caption = "line is unity\n black dot is centroid"
    )

```

Cross-congruency similarity contrast, table of parcel counts with significant effect ("TRUE"; uncorrected):

```{r echo = TRUE}

with(d.stats, table(interaction(measure, method), p.value < 0.05))

# parcels.eucl <- d.stats %>% filter(p.value < 0.05, measure == "eucl", method == "cross") %>% pull(parcel)
# parcels.corr <- d.stats %>% filter(p.value < 0.05, measure == "corr", method == "cross") %>% pull(parcel)
# parcels.neuc <- d.stats %>% filter(p.value < 0.05, measure == "neuc", method == "cross") %>% pull(parcel)
# 
# setdiff(parcels.eucl, parcels.corr)
# intersect(parcels.neuc, parcels.corr)
# setdiff(parcels.neuc, parcels.corr)
# setdiff(parcels.corr, parcels.neuc)

```


# 3. Extended analyses of prewhitening on cross-validated measures

## (a) between-subject correlations with alternative measures

### stroop effects (I--C discriminability) across item types (pc50, bias)

### association with RT

* averaged across DMCC34

## (b) noise ceilings

* dmcc34

