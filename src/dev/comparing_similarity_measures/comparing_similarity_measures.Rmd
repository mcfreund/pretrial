---
title: 'Comparing measures of pattern similarity using DMCC2 Stroop'
author: "mike freund"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    highlight: zenburn
---


# About

## Measures of similarity

Analyses within this document consider several different measures of similarity---12 measures in all.
I define these measures here.

In short, I consider three __types of similarity__: _linear correlation_, _Euclidean distance_, and _scaled Euclidean distance_.
For each type of similarity, I also consider two different __methods of estimation__: _between-run_ and via _cross-validation_.
Finally, I consider two different __types of normalization__: _"raw" (unnormalized)_, and spatially _pre-whitened_ (or "multivariate noise-normalized").
Thus, in all, I compare $3 * 2 * 2 = 12$ different measures of similarity.


### notation
  * $N_\text{vert}$: number of vertices in a given parcel.
  * $\mathbf{x}_{r}$: a row vector of beta estimates from run $r$ and condition $x$ for a given subject and parcel.
  This vector would contain $N_\text{vert}$ beta values, one for each vertex.
    * e.g., $\mathbf{x}_{1}$ would indicate the betas from run $1$.
  * $\mathbf{y}_{r}$: same as above, but for condition $y$
  * $\bar{x}_{r}$: the across-vertex, i.e., __parcel-wise__ mean for run $r$ and condition $x$ (i.e.,  $\mathbf{x}_{r}\mathbf{1}/N_\text{vert}$)

### linear correlation


#### between-run correlation

The Pearson correlation between two conditions ($x, y$) for scanning runs 1 and 2.


\[r_\textit{btw}(x_1, y_2) = \frac
{
(\mathbf{x}_{1} - \bar{x}_{1})
(\mathbf{y}_{2} - \bar{y}_{2})^T
}
{\sqrt{(\mathbf{x}_1 - \bar{x}_1)^2
(\mathbf{y}_2 - \bar{y}_2)^{2}}}\]

Or equivalently,

\[r_\textit{btw}(x_1, y_2) = 
\frac
{\text{cov}(\mathbf{x}_1, \mathbf{y}_2)}
{\sqrt{\text{var}({\mathbf{x}_1})\text{var}({\mathbf{y}_2})}}\]

* In other words, Pearson's can be thought of as the covariance between conditions $x$ and $y$ between run 1 and 2, scaled by the square-rooted product of their variances (i.e., the product of their standard deviations).

* The Pearson correlation is *biased*, as with increasing noise, it shrinks towards zero, away from the true (noise-free) value of the correlation.

#### cross-validated correlation

The cross-validated correlation between two conditions ($x, y$) for scanning runs 1 and 2.

\[r_\textit{cv}(x_1, y_2) = 
\frac
{\text{cov}(\mathbf{x}_1, \mathbf{y}_2)}
{\sqrt{\text{cov}({\mathbf{x}_1, \mathbf{x}_2})\text{cov}({\mathbf{y}_1, \mathbf{y}_2})}}\]

* The cross-validated correlation does not scale the covariance by the product of the standard deviations. 
Instead, it scales the covariance by the square-rooted product of the *cross-run covariances* within each condition.

* Think of the within-condition cross-run covariance as an estimate of the pattern relaibility.
Correlations between patterns with poor reliability will be *increased*.
This is closely related to the **spearman-brown prophecy** formula.

* The cross-validated correlation **is not a true correlation statistic**: it can range greater than 1 or -1, and can be undefined if the product of the cross-run covariances is negative (because of the square root).
Depending on the SNR, this potential for undefined similarities will likely pose a problem for group-level analyses, as there may be substantial portions of data missing.

* But, the upside to this measure is that it is *unbiased*: the expected value of this statistic (i.e., mean) is centered at the true value, regardless of the level of noise.

* This means that the expected value of the cross-validated correlation **does not depend on the amount of data** (i.e., number of trials in each GLM regressor).
In theory, this should allow us to compare different trial types *without downsampling / throwing out data*.

* note also that *two* correlations are now generated per pair of conditions (i.e., the similarity matrix is not symmetric).
This is because we can calculate $r_\textit{cv}(x_1, y_2)$ and $r_\textit{cv}(x_2, y_1)$.

### euclidean distance

#### between-run euclidean distance

The Euclidean distance between two conditions ($x, y$) for scanning runs 1 and 2.

\[d_\textit{btw}(x_1, y_2) = \sqrt{(\mathbf{x}_{1} - \mathbf{y}_{2})(\mathbf{x}_{1} - \mathbf{y}_{2})^T} / N_\text{vert}\]

* In other words, the root-sum-of-squares of the difference between $x$ in run 1 and $y$ in run 2.

* To state another way: __the difference vector $\mathbf{x}_1 - \mathbf{y}_2$ is multiplied to itself (dot product) then this scalar value is square-rooted.__
This can be thought of as yielding the *length of the difference vector*, i.e., distance from origin.

* NB: The formula above differs from the typical euclidean distance as I've divided by the number of dimensions, $N_{vert}$.
The typical euclidean distance increases with increasing dimenisons, so this division maintains comparaibility across parcels.

* This measure is also *biased*, as in the presence of noise, identical patterns would appear as distant (dissimilar).


#### cross-validated (squared) euclidean distance

The cross-validated (squared) Euclidean distance between two conditions ($x, y$) for scanning runs 1 and 2.

\[d^2_\textit{cv}(x_1, y_2) = (\mathbf{x}_{1} - \mathbf{y}_{1})(\mathbf{x}_{2} - \mathbf{y}_{2})^T / N_\text{vert}\]

* The cross-validated euclidean distance does not multiply the difference vector to itself.
Instead, it calculates the difference vectors *separately in each run*, then multiplies them *across runs*.

* The idea is that if there *is* a consistent difference between patterns, it should cross-validate across scanning runs (i.e., the dot product should be positive).
But, if the difference is purely noise, then the dot product should be, on average, zero (no successful crossvaliation).

* Thus, the cross-validated euclidean distance **is not a true distance metric**: it can range less than 0.
Note that, because it can be negative, the square-root is not taken and the distance $d$ is left in squared units ($d^2$).

* Like cross-validated correlation, this measure is also unbiased in the sense that the distance between indistinguishable patterns will have an expected value of zero.

* note also that, in contrast to the cross-validated correlation, a *single* distance measure is generated per pair of conditions (assuming two runs). I.e., cross-validated euclidean distance matrices *are* symmetric.

### scaled euclidean distance

* the scaled euclidean distance is equivalent to the euclidean distances above (both between-run and cross-validated), except that, prior to caclulating the distances,
the patterns are *z-score normalized* by subtracting the mean and dividing by the standard deviation of across-vertex betas.

* this makes the (squared) euclidean distance more-or-less equivalent to linear correlation (a quick google search will show the exact form of the relation).

* I computed scaled euclidean mostly as a sanity check.


### pre-whitening

Let $\mathbf{E}_r$ represent the matrix of residual timecourses for a given run $r$, parcel, and subject.
I.e., $\mathbf{E}_1$ is of dimension $N_{\text{TRs}} \times N_{\text{vertices}}$ and corresponds to run 1.

The vertex-by-vertex covariance matrix of $\mathbf{E}_1$ is given by $\mathbf{S}_1 = \mathbf{E}_1^T\mathbf{E}_1$.

These covariance matrices give the spatial correlation structure of the noise within a given parcel for each run. 
Each off-diagonal entry indicates the degree to which two vertices had residual time-courses that covaried during that run.

To pre-whiten beta coefficients $\mathbf{x}_1$, these covariance matrices are

  1. regularized by shrinking them towards an identity matrix by a factor $\lambda$, e.g.:

  \[\mathbf{S}^\textit{(shrnk.)}_1 = \lambda \mathbf{I} + (1 - \lambda) \mathbf{S}_1\]

  2. inverted: e.g.:

  \[\mathbf{S}^{\textit{(shrnk.)}-1}_1\]

  3. then averaged across runs and square-rooted

  \[[(\mathbf{S}^{\textit{(shrnk.)}-1}_1 + \mathbf{S}^{\textit{(shrnk.)}-1}_2) / 2]^{1/2}\]
  
This forms the (regularized) __mahalanobis prewhitening matrix__, __W__, which is applied to each beta estimate:

\[\mathbf{x}_1^{(\textit{prewhitened})} = \mathbf{x}_1\mathbf{W}\]

* For these these analyses, I used a shrinkage factor of $\lambda = 0.4$ for all parcels and subjects.

* Because I calculated cross-validated measures "by hand" (i.e., without using R's functions for euclidean distance and correlation),
I omitted the (time-consuming) square-root step for these measures, and applied the squared whitening matrix,  $\mathbf{W}^2 = [(\mathbf{S}^{\textit{(shrnk.)}-1}_1 + \mathbf{S}^{\textit{(shrnk.)}-1}_2) / 2]$, directly to the patterns within the similarity calculation.
For example:
\[d^2_\textit{cv, prewhitened}(x_1, y_2) = (\mathbf{x}_{1} - \mathbf{y}_{1})
\mathbf{W}^2
(\mathbf{x}_{2} - \mathbf{y}_{2})^T / N_\text{vert}\]


## Questions these analyses attempt to answer empirically


### 1. How does pre-whitening impact the sensitivity of RSA measures?

* effect sizes: the magnitude of the Stroop RSA contrast
  * non-crossvalidated measures: mean(PC50-I ~ PC50-I) - mean(PC50-I ~ PC50-C)
  * crossvalidated measures: PC50-I ~ PC50-C > 0 (euclidean distance) or PC50-I ~ PC50-C < 1 (correlation)

* effect sizes: the variability in the Stroop RSA contrast between subjects

* effect sizes: the test-statistic for Stroop RSA contrast (sum-of-signed ranks)

* significance / coverage of cortex: the set of parcels identified as significant

* convergent validity: correlation between subject's Stroop RSA contrast for PC50 items and Bias items
  * test would rely upon having measures that do not depend on amount of data.

* noise ceilings: inter-subject consistency in similarity matrices (representational geometry)

### 2. Was cross-validation successful at removing dependency on amount of data?

* Compare Bias-I ~ PC50-C and Bias-C ~ PC50-I between baseline and proactive
* dependency on amt of data: interaction, such that Bias-I is positive in proactive, Bias-C positive in baseline.
* no dependency would show as no interaction


